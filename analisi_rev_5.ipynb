{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi generale progetto Big Data, Alessandro Stefani, Cristi Gutu.\n",
    "### Il progetto tratta il problema della classificazione nella rispettiva categoria di news reperite dall'agenzia ANSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importazione funzioni processamento articoli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Cristy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from AC import preproc\n",
    "from AC import get_news\n",
    "import inspect\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ottenimento articoli per ogni categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2400/2400 [00:17<00:00, 134.37it/s]\n"
     ]
    }
   ],
   "source": [
    "economia = get_news(\"./articoli_economia/\")\n",
    "cultura = get_news(\"./articoli_cultura/\")\n",
    "tech = get_news(\"./articoli_tech/\")\n",
    "politica = get_news(\"./articoli_politica/\")\n",
    "sport = get_news(\"./articoli_sport/\")\n",
    "cronaca = get_news(\"./articoli_cronaca/\")\n",
    "\n",
    "for articolo in economia:\n",
    "    articolo['categoria'] = \"Economia\"\n",
    "for articolo in cultura:\n",
    "    articolo['categoria'] = \"Cultura\"\n",
    "for articolo in tech:\n",
    "    articolo['categoria'] = \"Tech\"\n",
    "for articolo in politica:\n",
    "    articolo['categoria'] = \"Politica\"\n",
    "for articolo in sport:\n",
    "    articolo['categoria'] = \"Sport\"\n",
    "for articolo in cronaca:\n",
    "    articolo['categoria'] = \"Cronaca\"\n",
    "dati_preprocessati =  preproc(tech + politica + cultura + economia + sport + cronaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AC import distribuzione_frequenze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importazione moduli necessari per la suddivisione dell'insieme di news in train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model,metrics\n",
    "from sklearn.dummy import DummyClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suddivisione news in insieme di training e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "seed = 2\n",
    "train_documents,test_documents = train_test_split(dati_preprocessati,random_state=seed, train_size = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costruzione della term document matrix e calcolo distribuzione Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-53a7069f43dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#distribuzione_frequenze(test_texts,\"test\",ngrammi=(1,6),min_df=11)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'minf' is not defined"
     ]
    }
   ],
   "source": [
    "docs_texts = [' '.join([word for word in x['testo']] + x['tags'] + x['sottotitolo'] + x['titolo_articolo']) for x in dati_preprocessati]\n",
    "train_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in train_documents]\n",
    "test_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in test_documents]\n",
    "\n",
    "docs_cats = [x[\"categoria\"] for x in dati_preprocessati]\n",
    "train_cats = [x[\"categoria\"] for x in train_documents]\n",
    "test_cats = [x[\"categoria\"] for x in test_documents]\n",
    "\n",
    "\n",
    "#distribuzione_frequenze(test_texts,\"test\",ngrammi=(1,6),min_df=11)\n",
    "print(minf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Nella figura si vede la distribuzione del numero di volte che una parola comapre nei documenti di training e di test\n",
    "##### Esempio: 6 parole compaiono in piu di 500 documenti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import tqdm\n",
    "\n",
    "train_documents,test_documents = train_test_split(dati_preprocessati,random_state=2, train_size = 0.5)\n",
    "test_documents,val_documents = train_test_split(test_documents,random_state=2, train_size = 0.5)\n",
    "\n",
    "train_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in train_documents]\n",
    "test_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in test_documents]\n",
    "val_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in val_documents]\n",
    "\n",
    "train_cats = [x[\"categoria\"] for x in train_documents]\n",
    "test_cats = [x[\"categoria\"] for x in test_documents]\n",
    "val_cats = [x[\"categoria\"] for x in val_documents]\n",
    "    \n",
    "ldac = Pipeline([\n",
    "    (\"count_mx\",CountVectorizer(encoding='utf-8',max_features=1000000, lowercase=True)),\n",
    "    (\"lda\", LatentDirichletAllocation(max_iter=50, learning_method='online',random_state=0)), \n",
    "    (\"classifier\",DecisionTreeClassifier(random_state=0 ))\n",
    "])\n",
    "\n",
    "param_grid = ParameterGrid({\n",
    "    'classifier__max_depth': np.arange(14, 18),\n",
    "    'classifier__min_samples_leaf':[1],# 2 ** np.arange(0,3),\n",
    "    'lda__n_components':[6, 12, 24],\n",
    "    'lda__learning_decay':[.5, .7, .9],\n",
    "    'count_mx__ngram_range':[(1,i) for i in range(1,15)],\n",
    "    'count_mx__min_df': np.arange(1, 25),\n",
    "    'count_mx__max_df': [60,70,0.5,0.6,0.7,0.8,0.9]\n",
    "})\n",
    "#print(param_grid.param_grid)\n",
    "param_list = list(ParameterSampler(param_grid.param_grid[0],n_iter=10,random_state=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "risultati = []\n",
    "\n",
    "for params in tqdm.tqdm(param_list):\n",
    "    ldac.set_params(**params)\n",
    "    ldac.fit(train_texts, train_cats)\n",
    "    y_pred = ldac.predict(val_texts)\n",
    "    params[\"accuracy_score\"] = metrics.accuracy_score(val_cats, y_pred)\n",
    "    risultati.append(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risultati = pd.DataFrame(risultati).sort_values([\"accuracy_score\",'count_mx__min_df','count_mx__max_df','lda__n_components','lda__learning_decay', \"classifier__max_depth\",], ascending=[False, True, True, True, True, True])\n",
    "risultati.reset_index(drop=True, inplace=True)\n",
    "print(\"Primi 5:\")\n",
    "display(risultati.head(20))\n",
    "\n",
    "print(\"Ultimi 5:\")\n",
    "risultati.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'lda__n_components': 6, 'lda__learning_decay': 0.7, 'count_mx__ngram_range': (1, 12), 'count_mx__min_df': 14, 'count_mx__max_df': 70, 'classifier__min_samples_leaf': 1, 'classifier__max_depth': 17}\n",
    "#{'lda__n_components': 8, 'lda__learning_decay': 0.9, 'count_mx__ngram_range': (1, 2), 'count_mx__min_df': 13, 'count_mx__max_df': 0.7, 'classifier__min_samples_leaf': 1, 'classifier__max_depth': 17}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldac.set_params(**params)\n",
    "ldac.fit(train_texts, train_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1,t2 = train_test_split(test_documents,random_state=42, train_size = 0.5)\n",
    "t1c = [x[\"categoria\"] for x in t1]\n",
    "t2c = [x[\"categoria\"] for x in t2]\n",
    "t1 = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in t1]\n",
    "t2 = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in t2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ldac.predict(t1)\n",
    "params[\"accuracy_score\"] = metrics.accuracy_score(t1c, y_pred)\n",
    "ris=[params]\n",
    "ris = pd.DataFrame(ris)\n",
    "display(ris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ldac.predict(t2)\n",
    "params[\"accuracy_score\"] = metrics.accuracy_score(t2c, y_pred)\n",
    "ris=[params]\n",
    "ris = pd.DataFrame(ris)\n",
    "display(ris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ldac.predict(test_texts)\n",
    "params[\"accuracy_score\"] = metrics.accuracy_score(test_cats, y_pred)\n",
    "ris=[params]\n",
    "ris = pd.DataFrame(ris)\n",
    "display(ris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuratezza al variare della dimensione del training set con e senza LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>train_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy_score  train_size\n",
       "0            0.85         0.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ldac = Pipeline([\n",
    "    (\"count_mx\",CountVectorizer(encoding='utf-8',max_features=1000000, lowercase=True)),\n",
    "    (\"classifier\",DecisionTreeClassifier(random_state=0))\n",
    "])\n",
    "params = {\n",
    "    'count_mx__ngram_range': (1, 12),\n",
    "    'count_mx__min_df': 13,\n",
    "    'count_mx__max_df': 0.9,\n",
    "    'classifier__min_samples_leaf': 1,\n",
    "    'classifier__max_depth': 15\n",
    "}\n",
    "ldac.set_params(**params)\n",
    "\n",
    "res = []\n",
    "for i in [0.8]:\n",
    "    train_documents,test_documents = train_test_split(dati_preprocessati,random_state=2, train_size = i)\n",
    "\n",
    "    train_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in train_documents]\n",
    "    test_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in test_documents]\n",
    "\n",
    "    train_cats = [x[\"categoria\"] for x in train_documents]\n",
    "    test_cats = [x[\"categoria\"] for x in test_documents]\n",
    "    \n",
    "    ldac.fit(train_texts, train_cats)\n",
    "    y_pred = ldac.predict(test_texts)\n",
    "    res.append({\"accuracy_score\":metrics.accuracy_score(test_cats, y_pred),\"train_size\": i})\n",
    "res = pd.DataFrame(res).sort_values([\"train_size\",\"accuracy_score\"], ascending=[True,False])\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>train_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.564352</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.769792</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.885119</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.913194</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.977500</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.980208</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy_score  train_size\n",
       "0        0.564352        0.10\n",
       "1        0.769792        0.20\n",
       "2        0.885119        0.30\n",
       "3        0.913194        0.40\n",
       "4        0.977500        0.50\n",
       "5        0.980208        0.60\n",
       "6        0.993333        0.75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ldac = Pipeline([\n",
    "    (\"count_mx\",CountVectorizer(encoding='utf-8',max_features=1000000, lowercase=True)),\n",
    "    (\"lda\", LatentDirichletAllocation(max_iter=50, learning_method='online',random_state=0)),\n",
    "    (\"classifier\",DecisionTreeClassifier(random_state=0))\n",
    "])\n",
    "params = {\n",
    "    'count_mx__ngram_range': (1, 8),\n",
    "    'count_mx__min_df': 13,\n",
    "    'count_mx__max_df': 0.9,\n",
    "    'lda__n_components':8,\n",
    "    'lda__learning_decay':0.5,\n",
    "    'classifier__min_samples_leaf': 1,\n",
    "    'classifier__max_depth': 15\n",
    "}\n",
    "ldac.set_params(**params)\n",
    "\n",
    "res = []\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.75]:\n",
    "    train_documents,test_documents = train_test_split(dati_preprocessati,random_state=2, train_size = i)\n",
    "\n",
    "    train_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in train_documents]\n",
    "    test_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in test_documents]\n",
    "\n",
    "    train_cats = [x[\"categoria\"] for x in train_documents]\n",
    "    test_cats = [x[\"categoria\"] for x in test_documents]\n",
    "\n",
    "    ldac.fit(train_texts, train_cats)\n",
    "    pred_cats = ldac.predict(test_texts)\n",
    "    res.append({\"accuracy_score\":metrics.accuracy_score(test_cats, pred_cats),\"train_size\": i})\n",
    "res = pd.DataFrame(res).sort_values([\"train_size\",\"accuracy_score\"], ascending=[True,False])\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Cronaca       1.00      1.00      1.00       127\n",
      "     Cultura       1.00      1.00      1.00       109\n",
      "    Economia       0.96      0.98      0.97       118\n",
      "    Politica       1.00      0.96      0.98       122\n",
      "       Sport       1.00      1.00      1.00       119\n",
      "        Tech       0.98      1.00      0.99       125\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       720\n",
      "   macro avg       0.99      0.99      0.99       720\n",
      "weighted avg       0.99      0.99      0.99       720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_cats, pred_cats, target_names=unique_labels(test_cats, pred_cats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "seed = 2\n",
    "train_documents,test_documents = train_test_split(dati_preprocessati,random_state=seed, train_size = 0.75)\n",
    "\n",
    "train_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in train_documents]\n",
    "test_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in test_documents]\n",
    "\n",
    "train_cats = [x[\"categoria\"] for x in train_documents]\n",
    "test_cats = [x[\"categoria\"] for x in test_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = CountVectorizer(ngram_range=(1,2), min_df=13,max_df=0.7,max_features=1000000, lowercase=True).fit(train_texts)\n",
    "tf_train = tf.transform(train_texts)\n",
    "ldax = LatentDirichletAllocation(n_components=8,max_iter=50,learning_decay=0.9, learning_method='online',random_state=0).fit(tf_train.toarray())\n",
    "ldax_train = ldax.transform(tf_train.toarray())\n",
    "dt = DecisionTreeClassifier(random_state=0, min_samples_leaf=1,max_depth=17).fit(ldax_train,train_cats)\n",
    "\n",
    "tf_test = tf.transform(test_texts)\n",
    "ldax_test = ldax.transform(tf_test.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 s ± 0 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "REPEAT = 30\n",
    "NUMBER = 1\n",
    "tempi = timeit.repeat(\"dt.predict(ldax_test)\",setup=\"from __main__ import dt,ldax_test\",repeat=REPEAT, number=NUMBER)\n",
    "print(\"{:.2f} s ± {:.0f} ms per loop (mean ± std. dev. of {} runs, {} loop each)\".format(\n",
    "    np.mean(tempi), 1000 * np.std(tempi), REPEAT, NUMBER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = CountVectorizer(ngram_range=(1,2), min_df=13,max_df=0.7,max_features=1000000, lowercase=True).fit(train_texts)\n",
    "tf_train = tf.transform(train_texts)\n",
    "tf_train = tf_train.toarray()\n",
    "dt = DecisionTreeClassifier(random_state=0, min_samples_leaf=1,max_depth=17).fit(tf_train,train_cats)\n",
    "\n",
    "tf_test = tf.transform(docs_texts)\n",
    "tf_test = tf_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02 s ± 1 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "REPEAT = 30\n",
    "NUMBER = 1\n",
    "tempi = timeit.repeat(\"dt.predict(tf_test)\",setup=\"from __main__ import dt,tf_test\",repeat=REPEAT, number=NUMBER)\n",
    "\n",
    "print(\"{:.2f} s ± {:.0f} ms per loop (mean ± std. dev. of {} runs, {} loop each)\".format(\n",
    "    np.mean(tempi), 1000 * np.std(tempi), REPEAT, NUMBER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
