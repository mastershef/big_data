{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.plotting import save\n",
    "\n",
    "import inspect\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from numpy import linalg\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "import nltk\n",
    "from os import listdir\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "\n",
    "# Importing sklearn and TSNE.\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "# We'll hack a bit with the t-SNE code in sklearn.\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.manifold.t_sne import (_joint_probabilities,\n",
    "                                    _kl_divergence)\n",
    "#from sklearn.utils.extmath import _ravel\n",
    "# Random state we define this random state to use this value in TSNE which is a randmized algo.\n",
    "RS = 25111993\n",
    "\n",
    "# Importing matplotlib for graphics.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import json\n",
    "from string import punctuation\n",
    "\n",
    "import string \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Importing seaborn to make nice plots.\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###FUNZIONI NECESSARIE PER ESTRAZIONE DATI DAGLI ARTICOLI PER POI CREARE LA TERM DOCUMENT MATRIX\n",
    "\n",
    "def news_analyzer(news, tokenizer, stemmer, stop_words):\n",
    "    # ============== YOUR CODE HERE ==============\n",
    "    campi_interesse = ['testo','titolo_articolo','sottotitolo']\n",
    "    for campo in campi_interesse:\n",
    "        #apostrofi fix\n",
    "        news[campo] = BeautifulSoup(news[campo], \"lxml\").get_text()\n",
    "        news[campo] = news[campo].replace(\"'\",\" \")\n",
    "        news[campo] = re.sub(\"http\\S+\", \" link \", news[campo])\n",
    "        news[campo] = tokenizer.tokenize(news[campo])\n",
    "        news[campo] = [token for token in news[campo] if not token.isdigit() and token not in stop_words]\n",
    "        #news[campo] = [stemmer.stem(token) for token in news[campo] if not token.isdigit() and token not in stop_words]\n",
    "    # ============================================\n",
    "\n",
    "    return news\n",
    "\n",
    "def preproc(news_dir):\n",
    "    nltk.download(\"stopwords\")\n",
    "    stop_words = stopwords.words('italian') + list(punctuation)\n",
    "    stemmer = SnowballStemmer(\"italian\")\n",
    "    tokenizer = TweetTokenizer(\n",
    "        preserve_case=False,\n",
    "        strip_handles=True\n",
    "    )\n",
    "    X_train =  []\n",
    "    onlyfiles = [f for f in listdir(news_dir) if isfile(join(news_dir, f))]\n",
    "    for articolo in onlyfiles:\n",
    "        docJson = open(news_dir+articolo,\"r\")\n",
    "        jsonData = json.loads(docJson.read())\n",
    "\n",
    "        X_train.append(jsonData)\n",
    "        docJson.close()\n",
    "\n",
    "    return [news_analyzer(news, tokenizer, stemmer, stop_words) for news in tqdm.tqdm(X_train)]\n",
    "\n",
    "# An user defined function to create scatter plot of vectors\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", 8))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(32, 32))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=120,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    #plt.xlim(-25, 25)\n",
    "    #plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # We add the labels for each cluster.\n",
    "    txts = []\n",
    "    for i in range(18):\n",
    "        # Position of each label.\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=50)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts\n",
    "def distribuzione_frequenze(tf):\n",
    "    tdf = np.sum(tf.toarray(), axis=0)\n",
    "    df ={}\n",
    "    for i in tdf:\n",
    "        if i in df:\n",
    "            df[i] += 1\n",
    "        else:\n",
    "            df[i] = 1\n",
    "\n",
    "    k = sorted(df.keys())\n",
    "    freq = [k,[]]\n",
    "    for x in freq[0]:\n",
    "        freq[1].append(df[x])\n",
    "    return (freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Cristy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 400/400 [00:00<00:00, 498.76it/s]\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Cristy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 400/400 [00:00<00:00, 425.26it/s]\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Cristy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 400/400 [00:00<00:00, 427.99it/s]\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Cristy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 400/400 [00:01<00:00, 351.94it/s]\n"
     ]
    }
   ],
   "source": [
    "economia = preproc(\"C:/Users/Cristy/Documents/Universita/big_data/articoli_economia/\")\n",
    "cultura = preproc(\"C:/Users/Cristy/Documents/Universita/big_data/articoli_cultura/\")\n",
    "tech = preproc(\"C:/Users/Cristy/Documents/Universita/big_data/articoli_tech/\")\n",
    "politica = preproc(\"C:/Users/Cristy/Documents/Universita/big_data/articoli_politica/\")\n",
    "for articolo in economia:\n",
    "    articolo['categoria'] = \"Economia\"\n",
    "for articolo in cultura:\n",
    "    articolo['categoria'] = \"Cultura\"\n",
    "for articolo in tech:\n",
    "    articolo['categoria'] = \"Tech\"\n",
    "for articolo in politica:\n",
    "    articolo['categoria'] = \"Politica\"\n",
    "dati_preprocessati =  tech + politica + cultura + economia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15, 16, 17, 20, 21, 22, 23, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 95, 96, 97, 98, 100, 102, 103, 104, 105, 106, 107, 110, 115, 116, 117, 118, 120, 121, 124, 125, 127, 128, 130, 138, 140, 142, 145, 150, 151, 152, 157, 160, 193, 257], [769, 136, 8, 524, 102, 10, 1, 358, 60, 17, 4, 236, 56, 15, 2, 1, 181, 39, 12, 5, 126, 32, 15, 2, 101, 27, 13, 3, 2, 82, 21, 5, 3, 60, 25, 7, 1, 4, 72, 16, 6, 5, 45, 17, 7, 3, 5, 38, 17, 7, 1, 32, 5, 3, 2, 2, 19, 6, 6, 3, 2, 18, 5, 2, 1, 1, 10, 7, 7, 1, 2, 2, 5, 1, 1, 2, 3, 1, 1, 6, 4, 1, 2, 1, 2, 1, 1, 3, 3, 1, 4, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "#####Costruisci la term document matrix e poi cerca\n",
    "documents = [' '.join([word for word in x['testo']] + x['tags'] + x['sottotitolo'] + x['titolo_articolo']) for x in dati_preprocessati if x['categoria']!= \"Cultura & Spettacoli\"]\n",
    "labels = np.array([x['categoria'] for x in dati_preprocessati])\n",
    "#print documents\n",
    "no_features = 300000\n",
    "tf_vectorizer = CountVectorizer(ngram_range=(1,12), min_df=14, max_df=70,encoding='utf-8',max_features=no_features, lowercase=True)\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "#create dataFrame\n",
    "df = pd.DataFrame(tf.toarray().transpose(), index = tf_vectorizer.get_feature_names())\n",
    "\n",
    "print(distribuzione_frequenze(tf))\n",
    "#'lda__n_components': 6, 'lda__learning_decay': 0.7, 'count_mx__ngram_range': (1, 12), 'count_mx__min_df': 14, 'count_mx__max_df': 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3497, 1600)\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creamo i cluster con LDA\n",
    "\n",
    "no_topics = 6\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=no_topics,max_iter=100,learning_decay=0.7, learning_method='online',random_state=0).fit(tf)\n",
    "\n",
    "#'lda__n_components': 6, 'lda__learning_decay': 0.7, 'count_mx__ngram_range': (1, 12), 'count_mx__min_df': 14, 'count_mx__max_df': 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_topics = lda.fit_transform(tf)\n",
    "#Correction\n",
    "threshold = 0.1\n",
    "_idx = np.amax(X_topics, axis=1) > threshold  # idx of doc that above the threshold\n",
    "X_topics = X_topics[_idx]\n",
    "X_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using t-SNE randomized algorithm\n",
    "tsne_model = TSNE(n_components=3, verbose=1, random_state=0, init='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 1600 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 1600 samples in 0.047s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1600\n",
      "[t-SNE] Computed conditional probabilities for sample 1600 / 1600\n",
      "[t-SNE] Mean sigma: 0.002335\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 51.630032\n",
      "[t-SNE] KL divergence after 1000 iterations: -1.186330\n"
     ]
    }
   ],
   "source": [
    "# N-D -> 2-D\n",
    "tsne_lda = tsne_model.fit_transform(X_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_topics = X_topics\n",
    "len(_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 colors\n",
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#bbc7e8\", \"#cc7f0e\", \"#ddbb78\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Then we find the most likely topic for each news:\n",
    "_lda_keys = []\n",
    "for i in range(X_topics.shape[0]):\n",
    "  _lda_keys +=  _topics[i].argmax(),\n",
    "\n",
    "len(_lda_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and get top words for each topic:\n",
    "n_top_words = 5\n",
    "topic_summaries = []\n",
    "topic_word = lda.components_  # all topic words\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "  topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words + 1):-1] # get!\n",
    "  topic_summaries.append(' '.join(topic_words)) # append!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\plotly\\graph_objs\\_deprecations.py:426: DeprecationWarning:\n",
      "\n",
      "plotly.graph_objs.Marker is deprecated.\n",
      "Please replace it with one of the following more specific types\n",
      "  - plotly.graph_objs.scatter.Marker\n",
      "  - plotly.graph_objs.histogram.selected.Marker\n",
      "  - etc.\n",
      "\n",
      "\n",
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\plotly\\graph_objs\\_deprecations.py:39: DeprecationWarning:\n",
      "\n",
      "plotly.graph_objs.Data is deprecated.\n",
      "Please replace it with a list or tuple of instances of the following types\n",
      "  - plotly.graph_objs.Scatter\n",
      "  - plotly.graph_objs.Bar\n",
      "  - plotly.graph_objs.Area\n",
      "  - plotly.graph_objs.Histogram\n",
      "  - etc.\n",
      "\n",
      "\n",
      "C:\\Users\\Cristy\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py:689: UserWarning:\n",
      "\n",
      "Consider using IPython.display.IFrame instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cristi.gutzu/5.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from plotly.plotly import iplot\n",
    "import plotly as pl\n",
    "from plotly.graph_objs import Scatter3d, Data, Marker\n",
    "\n",
    "# First three dimensions from reduced X VS the Y\n",
    "walkers = []\n",
    "\n",
    "trace0 = Scatter3d(\n",
    "    x=tsne_lda[0:199, 0],\n",
    "    y=tsne_lda[0:199, 1],\n",
    "    z=tsne_lda[0:199, 2], \n",
    "    marker=Marker(color=colormap[0], colorscale='Portland'),\n",
    "    mode='markers'\n",
    ")\n",
    "\n",
    "trace1 = Scatter3d(\n",
    "    x=tsne_lda[200:399, 0],\n",
    "    y=tsne_lda[200:399, 1],\n",
    "    z=tsne_lda[200:399, 2], \n",
    "    marker=Marker(color=colormap[1], colorscale='Portland'),\n",
    "    mode='markers'\n",
    ")\n",
    "\n",
    "trace2 = Scatter3d(\n",
    "    x=tsne_lda[400:599, 0],\n",
    "    y=tsne_lda[400:599, 1],\n",
    "    z=tsne_lda[400:599, 2], \n",
    "    marker=Marker(color=colormap[2], colorscale='Portland'),\n",
    "    mode='markers'\n",
    ")\n",
    "\n",
    "trace3 = Scatter3d(\n",
    "    x=tsne_lda[600:799, 0],\n",
    "    y=tsne_lda[600:799, 1],\n",
    "    z=tsne_lda[600:799, 2], \n",
    "    marker=Marker(color=colormap[3], colorscale='Portland'),\n",
    "    mode='markers'\n",
    ")\n",
    "\n",
    "walkers.append(trace0)\n",
    "walkers.append(trace1)\n",
    "walkers.append(trace2)\n",
    "walkers.append(trace3)\n",
    "\n",
    "data = Data(walkers) \n",
    "pl.tools.set_credentials_file(username='cristi.gutzu', api_key='S4SFAPdXM3dUxDF0wmxT')\n",
    "iplot(data, filename = 'pca-cloud')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Figure' object has no attribute 'surface3d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-47fbebf28abf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtooltips\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"content: @content - topic: @topic_key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mindex_cmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfactor_cmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'topic_key'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcats\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurface3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'z'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_cmap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Figure' object has no attribute 'surface3d'"
     ]
    }
   ],
   "source": [
    "## title = '1024 articoli ANSA'\n",
    "from bokeh.plotting import figure \n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "print(len(tsne_lda[:,0]))\n",
    "\n",
    "cats = [x['categoria'] for x in dati_preprocessati]\n",
    "testi = [x['testo'] for x in dati_preprocessati]\n",
    "data = { 'x' : tsne_lda[:,0],\n",
    "         'y' :  tsne_lda[:,1],\n",
    "         'z' :  tsne_lda[:,2],\n",
    "        'content': cats, \n",
    "        'topic_key': [str(x) for x in cats]}\n",
    " \n",
    "source = bp.ColumnDataSource(data)\n",
    "\n",
    "p = figure(plot_width=1100, plot_height=1100, tooltips=\"content: @content - topic: @topic_key\")\n",
    "index_cmap=factor_cmap('topic_key', palette=colormap, factors=list(set([str(x) for x in cats])) )\n",
    "p.surface3d('x', 'y','z', size=10, source=source, fill_color=index_cmap)\n",
    "\n",
    "\n",
    "show(p)\n",
    "list(set([str(x) for x in cats]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vogliamo un metodo per ottimizare il parametro del countVectorizer.\n",
    "#che e' ngarm range, questo parametro vive in N.\n",
    "\n",
    "from collections import Counter\n",
    "distr = {\"Tech\":[], \"Politica\":[], \"Economia\":[], \"Cultura\":[]}\n",
    "i=0\n",
    "for idex, doc in enumerate(dati_preprocessati):\n",
    "    topic = (_lda_keys[i])\n",
    "    distr[doc['categoria']].append(topic)\n",
    "    i=i+1\n",
    "def CountFrequency(my_list): \n",
    "  \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for item in my_list: \n",
    "        if (item in freq): \n",
    "            freq[item] += 1\n",
    "        else: \n",
    "            freq[item] = 1\n",
    "    lista = [item[1] for item in freq.items()]\n",
    "    for key, value in freq.items(): \n",
    "        print (\"% d : % d\"%(key, value)) \n",
    "        \n",
    "    return (max(lista)/sum(lista))\n",
    "    \n",
    "c1=CountFrequency(distr[\"Tech\"])\n",
    "print(\"#######\")\n",
    "c2=CountFrequency(distr[\"Economia\"])\n",
    "print(\"#######\")\n",
    "\n",
    "c3=CountFrequency(distr[\"Politica\"])\n",
    "print(\"#######\")\n",
    "\n",
    "c4=CountFrequency(distr[\"Cultura\"])\n",
    "\n",
    "indice = (c1+c2+c3+c4)/4\n",
    "print(indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# randomly choose a news (within a topic) coordinate as the crucial words coordinate\n",
    "topic_coord = np.empty((X_topics.shape[1], 2)) * np.nan\n",
    "for topic_num in _lda_keys:\n",
    "  if not np.isnan(topic_coord).any():\n",
    "    break\n",
    "  topic_coord[topic_num] = tsne_lda[_lda_keys.index(topic_num)]\n",
    "\n",
    "# plot crucial words\n",
    "for i in range(X_topics.shape[1]):\n",
    "  plot_lda.text(topic_coord[i, 0], topic_coord[i, 1], [topic_summaries[i]])\n",
    "\n",
    "# hover tools\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"content\": \"@content - topic: @topic_key\"}\n",
    "\n",
    "# save the plot\n",
    "save(plot_lda, '{}.html'.format(title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(bp.ColumnDataSource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
