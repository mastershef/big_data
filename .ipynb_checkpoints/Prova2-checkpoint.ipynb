{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AC import preproc\n",
    "from AC import get_news\n",
    "import inspect\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economia = get_news(\"./articoli_economia/\")\n",
    "cultura = get_news(\"./articoli_cultura/\")\n",
    "tech = get_news(\"./articoli_tech/\")\n",
    "politica = get_news(\"./articoli_politica/\")\n",
    "sport = get_news(\"./articoli_sport/\")\n",
    "cronaca = get_news(\"./articoli_cronaca/\")\n",
    "\n",
    "for articolo in economia:\n",
    "    articolo['categoria'] = \"Economia\"\n",
    "for articolo in cultura:\n",
    "    articolo['categoria'] = \"Cultura\"\n",
    "for articolo in tech:\n",
    "    articolo['categoria'] = \"Tech\"\n",
    "for articolo in politica:\n",
    "    articolo['categoria'] = \"Politica\"\n",
    "for articolo in sport:\n",
    "    articolo['categoria'] = \"Sport\"\n",
    "for articolo in cronaca:\n",
    "    articolo['categoria'] = \"Cronaca\"\n",
    "dati_preprocessati =  preproc(tech + politica + cultura + economia + sport + cronaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model,metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "train_documents,test_documents = train_test_split(dati_preprocessati,random_state=seed)\n",
    "test_documents, val_documents = train_test_split(test_documents,random_state=seed,train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_texts = [' '.join([word for word in x['testo']] + x['tags'] + x['sottotitolo'] + x['titolo_articolo']) for x in dati_preprocessati]\n",
    "train_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in train_documents]\n",
    "test_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in test_documents]\n",
    "val_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in val_documents]\n",
    "\n",
    "docs_cats = [x[\"categoria\"] for x in dati_preprocessati]\n",
    "train_cats = [x[\"categoria\"] for x in train_documents]\n",
    "test_cats = [x[\"categoria\"] for x in test_documents]\n",
    "val_cats = [x[\"categoria\"] for x in val_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificazione con LDA al variare della dimensione del training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldac = Pipeline([\n",
    "    (\"count_mx\",CountVectorizer(max_features=1000000, lowercase=True)),\n",
    "    (\"lda\", LatentDirichletAllocation(max_iter=50, learning_method='online',random_state=0)),\n",
    "    (\"classifier\",DecisionTreeClassifier(random_state=0))\n",
    "])\n",
    "params = {\n",
    "    'lda__n_components': 12,\n",
    "    'lda__learning_decay': 0.9, \n",
    "    'count_mx__ngram_range': (1, 3), \n",
    "    'count_mx__min_df': 13, \n",
    "    'count_mx__max_df': 80, \n",
    "    'classifier__min_samples_leaf': 1, \n",
    "    'classifier__max_depth': 20\n",
    "}\n",
    "ldac.set_params(**params)\n",
    "\n",
    "conLDA = []\n",
    "for i in [0.15,0.3,0.6]:\n",
    "    split = train_test_split(train_documents,random_state=0, train_size = i)\n",
    "    \n",
    "    tr_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in split[0]]\n",
    "    tr_cats = [x[\"categoria\"] for x in split[0]]\n",
    "    ldac.fit(tr_texts, tr_cats)\n",
    "    \n",
    "    pred_cats = ldac.predict(test_texts)\n",
    "    conLDA12.append({\"accuracy_score\":metrics.accuracy_score(test_cats, pred_cats),\"train_size\": i})\n",
    "\n",
    "# Training con tutto l'insieme\n",
    "ldac.fit(train_texts, tr_cats)\n",
    "pred_cats = ldac.predict(test_texts)\n",
    "conLDA12.append({\"accuracy_score\":metrics.accuracy_score(test_cats, pred_cats),\"train_size\": 1.0})\n",
    "    \n",
    "conLDA12 = pd.DataFrame(conLDA12).sort_values([\"train_size\",\"accuracy_score\"], ascending=[True,False])\n",
    "display(conLDA12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldac = Pipeline([\n",
    "    (\"count_mx\",CountVectorizer(max_features=1000000, lowercase=True)),\n",
    "    (\"lda\", LatentDirichletAllocation(max_iter=50, learning_method='online',random_state=0)),\n",
    "    (\"classifier\",DecisionTreeClassifier(random_state=0))\n",
    "])\n",
    "params = {\n",
    "    'lda__n_components': 48,\n",
    "    'lda__learning_decay': 0.9, \n",
    "    'count_mx__ngram_range': (1, 3), \n",
    "    'count_mx__min_df': 13, \n",
    "    'count_mx__max_df': 80, \n",
    "    'classifier__min_samples_leaf': 1, \n",
    "    'classifier__max_depth': 20\n",
    "}\n",
    "ldac.set_params(**params)\n",
    "\n",
    "conLDA = []\n",
    "for i in [0.15,0.3,0.6]:\n",
    "    split = train_test_split(train_documents,random_state=0, train_size = i)\n",
    "    \n",
    "    tr_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in split[0]]\n",
    "    tr_cats = [x[\"categoria\"] for x in split[0]]\n",
    "    ldac.fit(tr_texts, tr_cats)\n",
    "    \n",
    "    pred_cats = ldac.predict(test_texts)\n",
    "    conLDA48.append({\"accuracy_score\":metrics.accuracy_score(test_cats, pred_cats),\"train_size\": i})\n",
    "\n",
    "# Training con tutto l'insieme\n",
    "ldac.fit(train_texts, tr_cats)\n",
    "pred_cats = ldac.predict(test_texts)\n",
    "conLDA48.append({\"accuracy_score\":metrics.accuracy_score(test_cats, pred_cats),\"train_size\": 1.0})\n",
    "    \n",
    "conLDA48 = pd.DataFrame(conLDA48).sort_values([\"train_size\",\"accuracy_score\"], ascending=[True,False])\n",
    "display(conLDA48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificazione senza LDA al variare della dimensione del training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfc = Pipeline([\n",
    "    (\"count_mx\",CountVectorizer(max_features=1000000, lowercase=True)),\n",
    "    (\"classifier\",DecisionTreeClassifier(random_state=0))\n",
    "]) \n",
    "params = { \n",
    "    'count_mx__ngram_range': (1, 3), \n",
    "    'count_mx__min_df': 13, \n",
    "    'count_mx__max_df': 80, \n",
    "    'classifier__min_samples_leaf': 1, \n",
    "    'classifier__max_depth':  65\n",
    "}\n",
    "tfc.set_params(**params)\n",
    "\n",
    "senzaLDA = []\n",
    "for i in [0.15,0.3,0.6]:\n",
    "    split = train_test_split(train_documents,random_state=0, train_size = i)\n",
    "    \n",
    "    tr_texts = [' '.join([word for word in x['testo']] + x['sottotitolo'] + x['titolo_articolo']) for x in split[0]]\n",
    "    tr_cats = [x[\"categoria\"] for x in split[0]]\n",
    "    tfc.fit(tr_texts, tr_cats)\n",
    "\n",
    "    pred_cats = tfc.predict(test_texts)\n",
    "    senzaLDA.append({\"accuracy_score\":metrics.accuracy_score(test_cats, pred_cats),\"train_size\": i})\n",
    "\n",
    "# Training con tutto l'insieme\n",
    "tfc.fit(train_texts, tr_cats)\n",
    "pred_cats = tfc.predict(test_texts)\n",
    "senzaLDA.append({\"accuracy_score\":metrics.accuracy_score(test_cats, pred_cats),\"train_size\": 1.0})\n",
    "\n",
    "senzaLDA = pd.DataFrame(senzaLDA).sort_values([\"train_size\",\"accuracy_score\"], ascending=[True,False])\n",
    "display(senzaLDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple line plot\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.ylim((0,1))\n",
    "plt.plot( 'train_size', 'accuracy_score', data=conLDA12, marker='', linewidth=2)\n",
    "plt.plot( 'train_size', 'accuracy_score', data=conLDA48, marker='',color='green', linewidth=2)\n",
    "plt.plot( 'train_size', 'accuracy_score', data=senzaLDA, marker='', color='red', linewidth=2)\n",
    "plt.legend(['Con LDA-12', 'Con LDA-48', 'Senza LDA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
